---
description: Complete trophy testing workflow with automatic language detection. Maps codebase, analyzes dependencies, creates specs, generates language-specific integration tests, and runs appropriate code review.
---

# Trophy Workflow Command

Complete end-to-end workflow for verifying implementations using trophy testing methodology. **Automatically detects language and applies appropriate patterns and reviewers.**

## Quick Start

```bash
# 1. Run full workflow (auto-detects language)
/trophy-workflow run

# 2. Or specify language explicitly
/trophy-workflow run --lang go
/trophy-workflow run --lang python
/trophy-workflow run --lang typescript
/trophy-workflow run --lang java

# 3. Or run individual steps
/cartographer            # Map codebase → docs/CODEBASE_MAP.md
/deps                    # Analyze dependencies (auto-detects language)
/opsx:ff <feature>       # Create OpenSpec artifacts (proposal → design → tasks → specs)
/opsx:apply              # Implement tasks from specs
/trophy                  # Generate and run trophy tests from specs
```

---

## Language Detection

The workflow automatically detects the primary language by checking for:

| Indicator | Language | Test Patterns | Code Reviewer |
|-----------|----------|---------------|---------------|
| `go.mod` | Go | Table-driven, `go test` | `go-reviewer` |
| `pyproject.toml`, `setup.py`, `requirements.txt` | Python | pytest fixtures | `python-reviewer` |
| `package.json` | TypeScript/JavaScript | Jest/Vitest | `code-reviewer` |
| `pom.xml`, `build.gradle` | Java | JUnit, Testcontainers | `code-reviewer` |

**Override with `--lang`** if auto-detection picks the wrong language or for polyglot repos.

---

## Complete Tutorial: Trophy Testing a New Codebase

This tutorial walks through the complete process of setting up trophy testing for any codebase with automatic language-specific adaptation.

### Prerequisites

- Claude Code installed
- Git repository with source code
- Language-specific test dependencies installed

### Language-Specific Prerequisites

| Language | Requirements |
|----------|-------------|
| **Go** | Go 1.21+, test database |
| **Python** | Python 3.10+, pytest, test database |
| **TypeScript** | Node 18+, Jest or Vitest |
| **Java** | JDK 17+, Maven/Gradle, Testcontainers |

---

## Step 1: Configure Tree-sitter MCP Server

### One-Time Setup

Add the tree-sitter MCP server to your Claude Code configuration:

```bash
# Edit your Claude Code settings
# Location: ~/.claude/settings.json (global) or .claude/settings.json (project)
```

Add this to your settings:

```json
{
  "mcpServers": {
    "tree-sitter": {
      "command": "uvx",
      "args": ["mcp-tree-sitter"]
    }
  }
}
```

### Verify Setup

After restarting Claude Code, verify the MCP server is available:

```
/deps --help
```

You should see the tree-sitter-deps tool is available.

---

## Step 2: Generate Codebase Map with Cartographer

The codebase map documents your project's structure, entry points, and key dependencies. Use the **Cartographer** skill to automate this entirely.

### Run Cartographer

```
/cartographer
```

This will automatically:
1. **Scan** the codebase with `scan-codebase.py` to get file tree and token counts
2. **Plan** subagent assignments based on token budgets (~500k tokens per Sonnet agent)
3. **Spawn** parallel Sonnet subagents to read and analyze file groups
4. **Synthesize** all reports into comprehensive documentation
5. **Write** `docs/CODEBASE_MAP.md` with full architecture map
6. **Update** `CLAUDE.md` with a summary pointing to the map

### What Cartographer Produces

**`docs/CODEBASE_MAP.md`** includes:
- System overview with Mermaid architecture diagrams
- Annotated directory structure
- Module guide with entry points, exports, dependencies
- Data flow sequence diagrams
- Conventions and patterns used
- Gotchas and non-obvious behaviors
- Navigation guide for common tasks

### Update Mode

If `docs/CODEBASE_MAP.md` already exists, Cartographer will:
1. Check git history for changes since `last_mapped` timestamp
2. Only re-analyze changed modules
3. Merge updates with existing documentation

Just run `/cartographer` again to update.

### Example Output Structure

```markdown
# Codebase Map

> Auto-generated by Cartographer. Last mapped: 2024-01-15

## System Overview

[Mermaid diagram showing high-level architecture]

## Directory Structure

src/
├── main.py              # Application entry point
├── api/                 # HTTP route handlers
├── services/            # Business logic layer
├── models/              # Data models
└── db/                  # Database operations

## Module Guide

### API Layer
**Purpose**: HTTP request handling and validation
**Entry point**: api/routes.py
**Exports**: Route handlers for auth, users, teams
**Dependencies**: services layer, models

[... continues for each module ...]

## Data Flow

[Mermaid sequence diagrams for key flows]

## Navigation Guide

**To add a new API endpoint**: api/routes.py → services/{domain}.py → db/repository.py
**To modify auth**: api/auth.py → services/auth.py
```

### Prerequisites

Cartographer requires `tiktoken` for token counting:

```bash
# Preferred: UV handles it automatically
uv run scan-codebase.py

# Or install manually
pip install tiktoken
```

---

## Step 3: Analyze Dependencies with Tree-sitter

Use the `/deps` command to analyze actual code dependencies and validate your code map.

### Run Dependency Analysis

```
/deps src/
```

This will:
1. Parse all source files with tree-sitter
2. Extract imports, exports, and function calls
3. Build file-level and function-level dependency graphs
4. Report any discrepancies with `docs/CODEBASE_MAP.md`

### Example Output

```json
{
  "analysis": {
    "path": "src/",
    "language": "python",
    "files_analyzed": 8
  },
  "files": {
    "api/auth.py": {
      "imports": {
        "external": ["fastapi", "pydantic"],
        "internal": ["services.auth", "models.user"]
      },
      "exports": {
        "functions": ["login", "register", "refresh_token"]
      }
    }
  },
  "dependency_graph": {
    "file_level": {
      "api/auth.py": ["services/auth.py", "models/user.py"],
      "services/auth.py": ["db/repository.py", "models/user.py"]
    }
  }
}
```

### Validate Code Map

Compare the tree-sitter output against `docs/CODEBASE_MAP.md`:

```
Compare the dependency analysis against docs/CODEBASE_MAP.md and report:
1. Documented dependencies that don't exist in code
2. Actual dependencies missing from documentation
3. Recommended updates (or re-run /cartographer to refresh)
```

---

## Step 4: Create OpenSpec Artifacts with OPSX

Use `/opsx:ff` (fast-forward) to automatically generate all OpenSpec artifacts needed for implementation and testing.

### Run OPSX Fast-Forward

```bash
/opsx:ff <feature-name>
```

**Example:**
```bash
/opsx:ff add-user-authentication
```

### What OPSX Creates

The command automatically generates artifacts in dependency order:

```
openspec/changes/<feature-name>/
├── .openspec.yaml      # Schema metadata (auto-generated)
├── proposal.md         # Feature proposal (from your description)
├── design.md           # Architecture decisions
├── tasks.md            # Implementation task breakdown
└── specs/
    └── <component>/
        └── spec.md     # WHEN/THEN scenarios for trophy tests
```

### OPSX Artifact Flow

```
/opsx:ff "add user authentication"
        ↓
    1. Creates change directory
        ↓
    2. Generates proposal.md (what are we building?)
        ↓
    3. Generates design.md (how will we build it?)
        ↓
    4. Generates tasks.md (step-by-step implementation)
        ↓
    5. Generates specs/*.spec.md (WHEN/THEN scenarios)
        ↓
    "Ready for implementation! Run /opsx:apply"
```

### Spec Format (Auto-Generated)

OPSX generates spec.md files with WHEN/THEN scenarios:

```markdown
# Authentication Specification

## Requirements

### Requirement: User Login

#### Scenario: Successful login with valid credentials

- **WHEN** a user submits valid email and password
- **THEN** the system returns an authentication token
- **AND** the token expires in 24 hours

#### Scenario: Login fails with invalid password

- **WHEN** a user submits valid email but incorrect password
- **THEN** the system returns a 401 Unauthorized error
- **AND** the error message says "Invalid credentials"
```

### Alternative: Step-by-Step Artifact Creation

If you prefer more control, use individual OPSX commands:

```bash
/opsx:new <feature-name>      # Create change, show first artifact template
/opsx:continue                # Create next artifact in sequence
/opsx:continue                # Repeat until all artifacts done
```

---

## Step 5: Implement with OPSX Apply

After artifacts are created, implement the tasks defined in `tasks.md`.

### Run OPSX Apply

```bash
/opsx:apply
```

This will:
1. Read all context files (proposal, design, tasks, specs)
2. Loop through pending tasks in `tasks.md`
3. Implement each task with code changes
4. Mark tasks complete: `- [ ]` → `- [x]`
5. Continue until all tasks done or blocked

### Apply Output

```
## Implementing: add-user-authentication

Working on task 3/7: Create login endpoint
[...implementation...]
✓ Task complete

Working on task 4/7: Add password hashing
[...implementation...]
✓ Task complete

Progress: 7/7 tasks complete ✓
All tasks complete! Ready for trophy testing.
```

### If Implementation Reveals Issues

OPSX supports fluid workflow - if implementation reveals design issues:
- Pause and update artifacts (`design.md`, `specs/*.md`)
- Then continue with `/opsx:apply`

---

## Step 6: Generate and Run Trophy Tests (Language-Specific)

The workflow generates integration tests using **language-specific patterns** based on detected language.

### Run Trophy Testing

```
/trophy
```

This will:
1. Find all spec.md files in `openspec/changes/*/specs/`
2. Extract WHEN/THEN scenarios
3. Read source code and `docs/CODEBASE_MAP.md` for context
4. **Detect language** and apply appropriate test patterns
5. Generate integration tests for each scenario
6. Run tests with language-specific runner
7. Report results

### Language-Specific Test Generation

| Language | Pattern | Runner | Related Skill |
|----------|---------|--------|---------------|
| **Go** | Table-driven tests, subtests | `go test -race ./...` | `/go-trophy` |
| **Python** | pytest classes, fixtures | `pytest -v` | `/python-testing` |
| **TypeScript** | Jest/Vitest describe blocks | `npm test` | - |
| **Java** | JUnit 5, Testcontainers | `mvn test` | `/springboot-trophy` |

---

### Go Test Output (Table-Driven)

```go
// internal/services/auth_test.go
// Generated from: openspec/changes/auth/specs/authentication/spec.md

func TestUserLogin(t *testing.T) {
    db := testutil.SetupTestDB(t)
    repo := repository.NewUserRepo(db)
    svc := services.NewAuthService(repo)

    tests := []struct {
        name     string
        email    string
        password string
        wantErr  error
        scenario string
    }{
        {
            name:     "successful login with valid credentials",
            email:    "user@test.com",
            password: "SecurePass123!",
            wantErr:  nil,
            scenario: "auth/spec.md: Successful login",
        },
        {
            name:     "login fails with invalid password",
            email:    "user@test.com",
            password: "WrongPassword",
            wantErr:  services.ErrInvalidCredentials,
            scenario: "auth/spec.md: Invalid password",
        },
    }

    for _, tt := range tests {
        t.Run(tt.name, func(t *testing.T) {
            token, err := svc.Login(tt.email, tt.password)
            if !errors.Is(err, tt.wantErr) {
                t.Errorf("scenario %q: got %v, want %v", tt.scenario, err, tt.wantErr)
            }
            if tt.wantErr == nil && token == nil {
                t.Errorf("scenario %q: expected token", tt.scenario)
            }
        })
    }
}
```

**Run:** `go test -race -v ./...`

---

### Python Test Output (pytest)

```python
# tests/integration/test_auth.py
# Generated from: openspec/changes/auth/specs/authentication/spec.md

import pytest
from fastapi.testclient import TestClient
from app.main import app

@pytest.fixture
def client():
    return TestClient(app)

@pytest.fixture
def test_db():
    """Real test database"""
    db = create_test_database()
    yield db
    db.cleanup()


class TestUserLogin:
    """Scenarios from: authentication/spec.md - User Login"""

    def test_successful_login_with_valid_credentials(self, client, test_db):
        """
        WHEN a user submits valid email and password
        THEN the system returns an authentication token
        AND the token expires in 24 hours
        """
        create_user(test_db, email="user@test.com", password="SecurePass123!")

        response = client.post("/api/auth/login", json={
            "email": "user@test.com",
            "password": "SecurePass123!"
        })

        assert response.status_code == 200
        data = response.json()
        assert "access_token" in data
        assert data["expires_in"] == 86400

    def test_login_fails_with_invalid_password(self, client, test_db):
        """
        WHEN a user submits valid email but incorrect password
        THEN the system returns a 401 Unauthorized error
        """
        create_user(test_db, email="user@test.com", password="correct_password")

        response = client.post("/api/auth/login", json={
            "email": "user@test.com",
            "password": "wrong_password"
        })

        assert response.status_code == 401
        assert response.json()["detail"] == "Invalid credentials"
```

**Run:** `pytest -v tests/integration/`

---

### TypeScript Test Output (Jest/Vitest)

```typescript
// tests/integration/auth.test.ts
// Generated from: openspec/changes/auth/specs/authentication/spec.md

import { describe, it, expect, beforeEach, afterEach } from 'vitest'
import { createTestClient, setupTestDB, cleanupTestDB } from '../testutil'

describe('User Login', () => {
  let client: TestClient
  let db: TestDB

  beforeEach(async () => {
    db = await setupTestDB()
    client = createTestClient()
  })

  afterEach(async () => {
    await cleanupTestDB(db)
  })

  it('successful login with valid credentials', async () => {
    // Scenario: auth/spec.md - Successful login
    await createUser(db, { email: 'user@test.com', password: 'SecurePass123!' })

    const response = await client.post('/api/auth/login', {
      email: 'user@test.com',
      password: 'SecurePass123!'
    })

    expect(response.status).toBe(200)
    expect(response.data.access_token).toBeDefined()
    expect(response.data.expires_in).toBe(86400)
  })

  it('login fails with invalid password', async () => {
    // Scenario: auth/spec.md - Invalid password
    await createUser(db, { email: 'user@test.com', password: 'correct_password' })

    const response = await client.post('/api/auth/login', {
      email: 'user@test.com',
      password: 'wrong_password'
    })

    expect(response.status).toBe(401)
    expect(response.data.error).toBe('Invalid credentials')
  })
})
```

**Run:** `npm test` or `npx vitest`

---

### Java Test Output (JUnit 5 + Testcontainers)

```java
// src/test/java/com/example/auth/AuthServiceIntegrationTest.java
// Generated from: openspec/changes/auth/specs/authentication/spec.md

@SpringBootTest
@Testcontainers
class AuthServiceIntegrationTest {

    @Container
    static PostgreSQLContainer<?> postgres = new PostgreSQLContainer<>("postgres:15");

    @Autowired
    private AuthService authService;

    @Autowired
    private UserRepository userRepository;

    @BeforeEach
    void setUp() {
        userRepository.deleteAll();
    }

    @Test
    @DisplayName("Scenario: Successful login with valid credentials")
    void successfulLoginWithValidCredentials() {
        // WHEN a user submits valid email and password
        userRepository.save(new User("user@test.com", passwordEncoder.encode("SecurePass123!")));

        AuthToken token = authService.login("user@test.com", "SecurePass123!");

        // THEN the system returns an authentication token
        assertThat(token).isNotNull();
        assertThat(token.getAccessToken()).isNotBlank();
        // AND the token expires in 24 hours
        assertThat(token.getExpiresIn()).isEqualTo(86400);
    }

    @Test
    @DisplayName("Scenario: Login fails with invalid password")
    void loginFailsWithInvalidPassword() {
        // WHEN a user submits valid email but incorrect password
        userRepository.save(new User("user@test.com", passwordEncoder.encode("correct_password")));

        // THEN the system returns a 401 Unauthorized error
        assertThatThrownBy(() -> authService.login("user@test.com", "wrong_password"))
            .isInstanceOf(InvalidCredentialsException.class);
    }
}
```

**Run:** `mvn test` or `./gradlew test`

### Trophy Test Report

After running tests, you'll see a report like:

```
## Trophy Test Report

### Spec Coverage
| Spec File | Scenarios | Passed | Failed |
|-----------|-----------|--------|--------|
| auth/spec.md | 5 | 4 | 1 |
| users/spec.md | 6 | 6 | 0 |
| teams/spec.md | 4 | 3 | 1 |
| **Total** | **15** | **13** | **2** |

### Failed Scenarios
1. **auth/spec.md - Refresh expired token**
   - Expected: 401 Unauthorized
   - Actual: 500 Internal Server Error
   - Fix: Add expiration check in refresh_token()

2. **teams/spec.md - Remove member from team**
   - Expected: Member removed, notification sent
   - Actual: Member removed, no notification
   - Fix: Add notification in remove_member()

### Recommendations
- Fix the 2 failed scenarios before release
- Consider adding E2E test for login flow
```

---

## Step 7: Code Review (Language-Specific)

After tests pass, run the **language-specific code reviewer** for best practices.

### Automatic Reviewer Dispatch

| Language | Reviewer Agent | Focus Areas |
|----------|---------------|-------------|
| **Go** | `go-reviewer` | Idiomatic Go, concurrency, error handling |
| **Python** | `python-reviewer` | PEP 8, type hints, Pythonic idioms |
| **TypeScript** | `code-reviewer` | Types, async patterns, React best practices |
| **Java** | `code-reviewer` | Spring patterns, null safety, design patterns |

The workflow automatically invokes the appropriate reviewer based on detected language.

### Review Output Example

```
## Code Review: auth/services/auth.go

### CRITICAL
None found ✅

### HIGH
1. **Missing context.Context** (line 45)
   - `Login()` should accept context for cancellation
   - Fix: `func (s *AuthService) Login(ctx context.Context, email, password string)`

### MEDIUM
1. **Error wrapping** (line 52)
   - Wrap errors with context: `fmt.Errorf("login failed: %w", err)`

### Approved with suggestions
```

---

## Step 8: Fix and Iterate

### Fix Implementation Gaps

For each failing scenario:

1. Read the spec to understand expected behavior
2. Identify the code that needs to change
3. Implement the fix
4. Re-run tests with language-specific runner
5. Address code review feedback

### Language-Specific Test Commands

| Language | Run Tests | Run with Coverage |
|----------|-----------|-------------------|
| **Go** | `go test -race ./...` | `go test -cover -coverprofile=coverage.out ./...` |
| **Python** | `pytest -v` | `pytest --cov=src --cov-report=html` |
| **TypeScript** | `npm test` | `npm test -- --coverage` |
| **Java** | `mvn test` | `mvn test jacoco:report` |

### Iterate Until All Scenarios Pass

Continue until you see:

```
Trophy Test Results:
✅ 15/15 scenarios verified
All specs passing!

Code Review:
✅ No CRITICAL or HIGH issues
```

---

## Complete Workflow Summary

```
┌─────────────────────────────────────────────────────────────┐
│      TROPHY TESTING WORKFLOW (Language-Aware + OPSX)         │
└─────────────────────────────────────────────────────────────┘

Step 0: DETECT LANGUAGE (Automatic)
├── Check for go.mod → Go
├── Check for pyproject.toml/setup.py → Python
├── Check for package.json → TypeScript/JavaScript
├── Check for pom.xml/build.gradle → Java
└── Override with --lang flag if needed

Step 1: SETUP (One-time)
├── Configure tree-sitter MCP server
└── Install language-specific test dependencies

Step 2: MAP CODEBASE
├── Run /cartographer
├── Parallel Sonnet subagents analyze all files
├── Generates docs/CODEBASE_MAP.md
└── Updates CLAUDE.md with summary

Step 3: ANALYZE DEPENDENCIES
├── Run /deps (auto-detects language)
├── Compare against CODEBASE_MAP.md
└── Validate import graphs

Step 4: CREATE OPENSPEC ARTIFACTS (OPSX)
├── Run /opsx:ff <feature-name>
├── Generates proposal.md → design.md → tasks.md
├── Generates specs/*.spec.md with WHEN/THEN scenarios
└── "Ready for implementation!"

Step 5: IMPLEMENT (OPSX)
├── Run /opsx:apply
├── Reads context (proposal, design, tasks, specs)
├── Implements each task from tasks.md
└── Marks tasks complete as it goes

Step 6: GENERATE & RUN TROPHY TESTS (Language-Specific)
├── Run /trophy
├── Detect language → apply patterns:
│   ├── Go: table-driven tests, go test -race
│   ├── Python: pytest fixtures, pytest -v
│   ├── TypeScript: Jest/Vitest describe blocks
│   └── Java: JUnit 5, Testcontainers
├── Generate integration tests from specs
└── Run tests and report results

Step 7: CODE REVIEW (Language-Specific)
├── Dispatch to appropriate reviewer:
│   ├── Go → go-reviewer agent
│   ├── Python → python-reviewer agent
│   └── Other → code-reviewer agent
└── Address CRITICAL and HIGH issues

Step 8: FIX & ITERATE
├── Fix failing scenarios
├── Address review feedback
├── Re-run tests
└── Repeat until all pass

┌─────────────────────────────────────────────────────────────┐
│                         DONE!                                │
│   All scenarios verified ✅  Code review passed ✅           │
└─────────────────────────────────────────────────────────────┘
```

---

## Quick Reference Commands

### Core Workflow

| Command | Purpose |
|---------|---------|
| `/trophy-workflow run` | Run full workflow with auto language detection |
| `/trophy-workflow run --lang go` | Run workflow for Go project |
| `/cartographer` | Auto-generate `docs/CODEBASE_MAP.md` |
| `/deps <path>` | Analyze code dependencies (auto-detects language) |
| `/opsx:ff <feature>` | Create all OpenSpec artifacts (proposal → design → tasks → specs) |
| `/opsx:apply` | Implement tasks from OpenSpec change |
| `/opsx:new <feature>` | Start new change (step-by-step artifact creation) |
| `/opsx:continue` | Continue creating next artifact |
| `/trophy` | Generate and run trophy tests from specs |

### Language-Specific Skills

| Language | Trophy Skill | Review Skill | Build Resolver |
|----------|-------------|--------------|----------------|
| **Go** | `/go-trophy` | `/go-review` | `/go-build` |
| **Python** | `/python-testing` | `/python-review` | - |
| **TypeScript** | `/trophy` | - | - |
| **Java** | `/springboot-trophy` | - | - |

### Test Runners

| Language | Command |
|----------|---------|
| **Go** | `go test -race -v ./...` |
| **Python** | `pytest -v` |
| **TypeScript** | `npm test` |
| **Java** | `mvn test` |

---

## Tips for Success

### Writing Good Specs

1. **Be specific** - "Returns 401" not "Returns error"
2. **Test boundaries** - Empty inputs, max values, edge cases
3. **Cover errors** - Every happy path needs error scenarios
4. **Use real examples** - Concrete data, not placeholders

### What to Mock vs Use Real

| Always Real (Don't Mock) | Always Mock |
|-------------------------|-------------|
| Your own database | External APIs (Stripe, OpenAI) |
| Your own HTTP endpoints | Third-party services |
| Your own file system | Email/SMS providers |
| Internal modules | Payment gateways |

### Language-Specific Test Helpers

| Language | Test DB Setup | HTTP Testing | Fixtures |
|----------|--------------|--------------|----------|
| **Go** | `testutil.SetupTestDB(t)` | `httptest.NewRecorder()` | `//go:embed testdata/` |
| **Python** | `@pytest.fixture` + sqlalchemy | `TestClient(app)` | `conftest.py` |
| **TypeScript** | `beforeEach` + prisma | `supertest` | `__fixtures__/` |
| **Java** | `@Testcontainers` | `MockMvc` | `@Sql` annotations |

### Trophy Test Distribution

```
       /\
      /E2E\      <- 10%: Critical user journeys only
     /------\
    /INTEGR. \   <- 70%: Real DB, real HTTP, language patterns
   /----------\
  /   UNIT    \  <- 20%: Complex pure functions only
 /--------------\
|    STATIC     | <- Free: Linters, type checkers
------------------
```

### When to Add Unit Tests

Only add unit tests for:
- Complex mathematical calculations
- Data transformation functions with many edge cases
- Business rule validation with clear inputs/outputs

Skip unit tests when:
- Integration tests already cover the code
- The function just delegates to other functions
- It's a simple getter/setter

---

## Troubleshooting

### "Language not detected"

Run with explicit language flag:
```bash
/trophy-workflow run --lang go
/trophy-workflow run --lang python
```

Or run `/deps` with explicit language:
```
/deps src/ --language python
```

### "No spec files found"

Ensure your spec files are in the correct location:
```
openspec/changes/{feature}/specs/{component}/spec.md
```

### "Grammar not available"

For unsupported languages, generate a grammar first:
```
/grammar rpgle src/as400/
```

### Language-Specific Issues

#### Go
```bash
# Database connection failed
export TEST_DATABASE_URL="postgres://localhost:5432/myapp_test?sslmode=disable"

# Race condition detected
go test -race -v ./... 2>&1 | grep -A 20 "DATA RACE"

# Module not found
go mod tidy
```

#### Python
```bash
# Import errors
pip install -e .
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# Fixture not found
# Ensure conftest.py is in tests/ directory

# Database connection
export TEST_DATABASE_URL="postgresql://localhost:5432/test_db"
```

#### TypeScript
```bash
# Module resolution
npm install
npx tsc --noEmit

# Jest config issues
npx jest --showConfig
```

#### Java
```bash
# Testcontainers failing
# Ensure Docker is running
docker info

# Spring context not loading
# Check @SpringBootTest annotation and component scan
```

### "Tests failing for unknown reason"

1. Check the generated test file for syntax errors
2. Verify fixtures/test helpers are set up correctly
3. Ensure test database is accessible and migrated
4. Check for missing environment variables
5. Run tests in verbose mode for detailed output
